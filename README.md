# ğŸ§  AutoEval Lab  
### An Agentic AI Evaluation Framework (Local & Free)

AutoEval Lab is an **autonomous evaluation framework** for local LLMs (e.g., Ollama models like Gemma or Qwen).  
It automatically generates test problems, evaluates model responses, and tracks performance trends â€”  
mimicking how â€œagenticâ€ AI systems **reflect, assess, and improve** themselves over time.

---

## ğŸš€ Features

âœ… **Fully local** â€” works with Ollama (no paid APIs needed)  
âœ… **Agentic architecture** â€” multiple agents (Dataset, Evaluator, Judge, Memory) coordinate to evaluate models  
âœ… **Supports multiple domains** â€” Math & Reasoning  
âœ… **Dynamic difficulty** â€” test adapts based on model accuracy  
âœ… **Evaluation memory** â€” tracks all runs with long-term statistics  
âœ… **Dashboard & charts** â€” visualize model improvement trends  

---

## ğŸ§© Architecture Overview

The system is composed of autonomous components:

| Component | Role |
|------------|------|
| `dataset_agent` | Generates math or reasoning test problems |
| `ollama_client` | Sends prompts to a local model (Gemma, Qwen, etc.) |
| `evaluator_agent` | Grades answers automatically or with LLM-as-judge |
| `memory` | Logs accuracy trends across runs |
| `governance` | Enforces run limits & safety rules |
| `plots` / `dashboard` | Visualizes accuracy trends and past experiments |

Data flow:

```
Dataset Agent â†’ Model (Ollama) â†’ Evaluator â†’ Report â†’ Memory â†’ Dashboard
```

---

## ğŸ§ª Usage

### 1ï¸âƒ£ Run a quick evaluation
```bash
python app.py --n 12 --model qwen2.5:0.5b-instruct
```
Example output:
```
=== REPORT ===
{'accuracy': 0.91, 'n': 12}
```

### 2ï¸âƒ£ Run a multi-round experiment
Automatically adjusts difficulty and tracks progress.
```bash
python app.py --rounds 5 --domain math
```

### 3ï¸âƒ£ Generate trend plots
```bash
python -m plots.plot_trend
```
Outputs: `experiments/accuracy_trend.png`

### 4ï¸âƒ£ Launch the dashboard
```bash
streamlit run dashboard/app.py
```

Explore:
- Accuracy over time
- Run-level reports
- Detailed answers per question

---

## ğŸ“Š Example Output Files

All experiments are saved under `/experiments/`:

| File | Description |
|------|--------------|
| `*_benchmark.json` | Generated test set |
| `*_records.json` | Model answers |
| `*_report.json` | Summary metrics |
| `*_report.md` | Human-readable report |
| `index.json` | Cumulative history for trend analysis |

---

## ğŸ§  How It Works

1. **Task Generation:** Creates math or logic problems.  
2. **Model Evaluation:** Sends tasks to local LLMs via Ollama.  
3. **Auto-Grading:** Scores answers automatically (math) or with judge model (reasoning).  
4. **Memory Logging:** Saves results to `experiments/index.json`.  
5. **Self-Reflection (Next Step):** Plans harder/easier tests for next rounds.  

---

## ğŸ§© Example Extensions

Future ideas (for advanced users):
- Self-correction loops (model reflects on its wrong answers)
- Prompt refinement agents
- Bias & robustness testing
- Fine-tuning using LoRA adapters
- Multi-model comparison tournaments

---

## ğŸ§° Requirements

- **Python â‰¥ 3.9**
- **Ollama** (running locally)
- Dependencies:
  ```bash
  pip install typer aiohttp streamlit matplotlib pandas
  ```

---

## ğŸ“„ Folder Structure

```
auto-eval-lab/
â”‚
â”œâ”€â”€ app.py                # CLI entrypoint
â”œâ”€â”€ orchestrator.py       # Controls evaluation rounds
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ dataset_agent.py
â”‚   â”œâ”€â”€ evaluator_agent.py
â”‚   â””â”€â”€ analyst_agent.py
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ governance.py
â”‚   â”œâ”€â”€ memory.py
â”‚   â”œâ”€â”€ io.py
â”‚   â””â”€â”€ scoring.py
â”œâ”€â”€ models/
â”‚   â””â”€â”€ ollama_client.py
â”œâ”€â”€ plots/
â”‚   â””â”€â”€ plot_trend.py
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py
â””â”€â”€ experiments/          # Output folder (auto-created)
```

---

## ğŸ’¡ Project Pitch (for README/Resume)

> â€œBuilt an autonomous local AI evaluation framework that benchmarks LLM reasoning and math capabilities using a multi-agent architecture. Includes dataset generation, automatic grading, trend tracking, and visualization â€” fully local (Ollama-based) and self-improving through adaptive feedback loops.â€

---

## ğŸ§¾ License
MIT License Â© 2025 Abhishek Dhaka

---
